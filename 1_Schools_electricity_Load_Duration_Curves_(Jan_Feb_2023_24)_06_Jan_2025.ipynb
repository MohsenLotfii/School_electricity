{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohsenLotfii/School_electricity/blob/main/1_Schools_electricity_Load_Duration_Curves_(Jan_Feb_2023_24)_06_Jan_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-NM9RL5_xu1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2835da5c-47ae-455e-d70c-bc0bf23815bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52fJrE3b78rE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#############################################################################################################\n",
        "\n",
        "# Floor areas for each school\n",
        "floorAreas = {\n",
        "    \"Alpha\": 7742,\n",
        "    \"Curé-Paquin\": 4579,\n",
        "    \"Des_Perséides,pavillon_des_Primevères\": 3108,\n",
        "    # \"Gaston-Pilon(300209825)\": 5103,\n",
        "    # \"Gaston-Pilon(300206515)\": 5103,\n",
        "    # \"Gaston-Pilon(300205293)\": 5103,\n",
        "    \"De_la_Clé-des-Champs\": 2999,\n",
        "    \"Horizon-du-Lac\": 4977,\n",
        "    \"Marie-Soleil-Tougas\": 5478,\n",
        "    \"École_Domaine_Vert-Nord\": 5717,\n",
        "    \"Sauvé\": 3842,\n",
        "    # \"Village-des-Jeunes (300206655)\": 2306,\n",
        "    # \"Village-des-Jeunes (300206667)\": 2306,\n",
        "    #\"Terre des jeunes\": 3710,\n",
        "    \"Notre-Dame\": 3512,\n",
        "    \"Secondaire Rive-Nord\": 9780,\n",
        "    \"Sainte-Scholastique\": 2442,\n",
        "    \"De la Clairière\": 3220,\n",
        "    \"Jeunes du monde\": 3598,\n",
        "    \"De l'Aquarelle\": 3220,\n",
        "    \"Le Sentier\": 3235,\n",
        "    \"Secondaire Lucille-Teasdale\": 9899,\n",
        "    \"Saint-Gabriel (pavillon Saint-Gabriel)\": 6205,\n",
        "    \"De Fontainebleau\": 4275,\n",
        "    \"Arthur-Vaillancourt\": 2626,\n",
        "    \"Des Moissons (pavillon Mgr Conrad-Chaumont)\": 2957,\n",
        "    \"Secondaire du Harfang (pavillon Saint-François)\": 3609,\n",
        "    \"Secondaire d'Oka\": 16013,\n",
        "    \"Secondaire Liberté-Jeunesse\": 8405,\n",
        "    \"Au Coeur-du-Boisé\": 3157,\n",
        "    \"Secondaire Jean-Jacques-Rousseau\": 13148,\n",
        "    \"Des Mésanges\": 3929,\n",
        "    \"Des Perséides\": 4252,\n",
        "    \"Secondaire du Harfang\": 8234,\n",
        "    \"De l'Harmonie-Jeunesse\": 2825,\n",
        "    }\n",
        "\n",
        "# Mechanical equipment details for each school (GB: Gas Boiler, EB: Electric Boiler, TES: Thermal Energy Storage, HP: Heat Pump, GT: Geothermal)\n",
        "equipment = {\n",
        "    \"Alpha\": {\"GB\": 3, \"EB\": 1, \"TES\": 1, \"HP\": 0, \"GT\": 0},\n",
        "    \"Curé-Paquin\": {\"GB\": 0, \"EB\": 1, \"TES\": 1, \"HP\": 0, \"Radiant Heating\": 1, \"PV Panel\": 1, \"GT\": 0},\n",
        "    \"Des_Perséides,pavillon_des_Primevères\": {\"GB\": 0, \"EB\": 2, \"TES\": 1, \"HP\": 0, \"GT\": 0},\n",
        "    # \"Gaston-Pilon(300209825)\": {\"GB\": 1, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    # \"Gaston-Pilon(300206515)\": {\"GB\": 1, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    # \"Gaston-Pilon(300205293)\": {\"GB\": 1, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"De_la_Clé-des-Champs\": {\"GB\": 0, \"EB\": 1, \"TES\": 0, \"HP\": 1, \"GT\": 0},\n",
        "    \"Horizon-du-Lac\": {\"GB\": 1, \"EB\": 1, \"TES\": 1, \"HP\": 36, \"GT\": 0},\n",
        "    \"Marie-Soleil-Tougas\": {\"GB\": 0, \"EB\": 2, \"TES\": 1, \"HP\": 0, \"GSHP\":1, \"GT\": 0, \"RF\": 1},\n",
        "    \"École_Domaine_Vert-Nord\": {\"GB\": 0, \"EB\": 0, \"TES\": 0, \"HP\": 1, \"GT\": 0},\n",
        "    \"Sauvé\": {\"GB\": 2, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    # \"Village-des-Jeunes (300206655)\": {\"GB\": 1, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    # \"Village-des-Jeunes (300206667)\": {\"GB\": 1, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    #\"Terre des jeunes\": {\"GB\": 2, \"EB\": 0, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Notre-Dame\": {\"GB\": 1, \"EB\": 2, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Secondaire Rive-Nord\": {\"GB\": 3, \"EB\": 0, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Sainte-Scholastique\": {\"GB\": 0, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 1},\n",
        "    \"De la Clairière\": {\"GB\": 0, \"EB\": 0, \"OB\":1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Jeunes du monde\": {\"GB\": 0, \"EB\": 2, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"De l'Aquarelle\": {\"GB\": 0, \"EB\": 2, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Le Sentier\": {\"GB\": 2, \"EB\": 2, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Secondaire Lucille-Teasdale\": {\"GB\": 2, \"EB\": 0, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Saint-Gabriel (pavillon Saint-Gabriel)\": {\"GB\": 1, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"De Fontainebleau\": {\"GB\": 1, \"EB\": 0, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Arthur-Vaillancourt\": {\"GB\": 1, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Des Moissons (pavillon Mgr Conrad-Chaumont)\": {\"GB\": 1, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Secondaire du Harfang (pavillon Saint-François)\": {\"GB\": 2, \"EB\": 0, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Secondaire d'Oka\": {\"GB\": 2, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Secondaire Liberté-Jeunesse\": {\"GB\": 2, \"EB\": 0, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Au Coeur-du-Boisé\": {\"GB\": 0, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GSHP\":1, \"GT\": 0},\n",
        "    \"Secondaire Jean-Jacques-Rousseau\": {\"GB\": 2, \"EB\": 0, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Des Mésanges\": {\"GB\": 2, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Des Perséides\": {\"GB\": 0, \"EB\": 1, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"Secondaire du Harfang\": {\"GB\": 2, \"EB\": 0, \"TES\": 0, \"HP\": 0, \"GT\": 0},\n",
        "    \"De l'Harmonie-Jeunesse\": {\"GB\": 1, \"EB\": 0, \"TES\": 0, \"HP\": 2, \"GT\": 0},\n",
        "}\n",
        "\n",
        "# Softer, more visually friendly colors for each school\n",
        "school_colors = {\n",
        "    \"Alpha\": \"#2E8B57\",\n",
        "    \"Curé-Paquin\": \"#1E90FF\",\n",
        "    \"Des_Perséides,pavillon_des_Primevères\": \"#FF6347\",\n",
        "    # \"Gaston-Pilon(300209825)\": \"#DAA520\",\n",
        "    # \"Gaston-Pilon(300206515)\": \"#8A2BE2\",\n",
        "    # \"Gaston-Pilon(300205293)\": \"#20B2AA\",\n",
        "    \"De_la_Clé-des-Champs\": \"#FF4500\",\n",
        "    \"Horizon-du-Lac\": \"#4682B4\",\n",
        "    \"Marie-Soleil-Tougas\": \"#6A5ACD\",\n",
        "    \"École_Domaine_Vert-Nord\": \"#32CD32\",\n",
        "    \"Sauvé\": \"#00BFFF\",\n",
        "    # \"Village-des-Jeunes (300206655)\": \"#3CB371\",\n",
        "    # \"Village-des-Jeunes (300206667)\": \"#DC143C\",\n",
        "    #\"Terre des jeunes\": \"#DA70D6\",\n",
        "    \"Notre-Dame\": \"#FF8C00\",\n",
        "    \"Secondaire Rive-Nord\": \"#9370DB\",\n",
        "    \"Sainte-Scholastique\": \"#B22222\",\n",
        "    \"De la Clairière\": \"#F4A460\",\n",
        "    \"Jeunes du monde\": \"#BDB76B\",\n",
        "    \"De l'Aquarelle\": \"#483D8B\",\n",
        "    \"Le Sentier\": \"#FF7F50\",\n",
        "    \"Secondaire Lucille-Teasdale\": \"#7B68EE\",\n",
        "    \"Saint-Gabriel (pavillon Saint-Gabriel)\": \"#556B2F\",\n",
        "    \"De Fontainebleau\": \"#CD853F\",\n",
        "    \"Arthur-Vaillancourt\": \"#8B008B\",\n",
        "    \"Des Moissons (pavillon Mgr Conrad-Chaumont)\": \"#BC8F8F\",\n",
        "    \"Secondaire du Harfang (pavillon Saint-François)\": \"#4682B4\",\n",
        "    \"Secondaire d'Oka\": \"#228B22\",\n",
        "    \"Secondaire Liberté-Jeunesse\": \"#CD5C5C\",\n",
        "    \"Au Coeur-du-Boisé\": \"#008B8B\",\n",
        "    \"Secondaire Jean-Jacques-Rousseau\": \"#A0522D\",\n",
        "    \"Des Mésanges\": \"#4169E1\",\n",
        "    \"Des Perséides\": \"#FF69B4\",\n",
        "    \"Secondaire du Harfang\": \"#708090\",\n",
        "    \"De l'Harmonie-Jeunesse\": \"#DAA520\",\n",
        "}\n",
        "\n",
        "############################################ Load duration curves ##############################################\n",
        "# Function to generate Load Duration Curves (LDCs)\n",
        "def generate_load_duration_curve(data, title=\"Load Duration Curve\", year=None, cluster_num=None):\n",
        "    \"\"\"\n",
        "    Generates and plots a Load Duration Curve (LDC) from a dataset of loads.\n",
        "\n",
        "    Parameters:\n",
        "    - data: pandas.Series or numpy array of load values.\n",
        "    - title: str, title of the LDC plot.\n",
        "    - year: int, optional, the year of the data (for plot annotation).\n",
        "    - cluster_num: int, optional, the cluster number (for plot annotation).\n",
        "\n",
        "    Returns:\n",
        "    None: Displays the plot.\n",
        "    \"\"\"\n",
        "    if len(data) == 0:\n",
        "        print(\"No data available for Load Duration Curve.\")\n",
        "        return\n",
        "\n",
        "    # Sort the load values in descending order\n",
        "    sorted_data = np.sort(data)[::-1]\n",
        "    # Generate the percentage of time for the x-axis\n",
        "    relative_duration = np.linspace(0, 100, len(sorted_data))\n",
        "\n",
        "    # Plotting the Load Duration Curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(relative_duration, sorted_data, label=f\"LDC (Cluster {cluster_num})\", color=\"blue\", linewidth=2)\n",
        "    plt.title(f\"{title} - Year {year} - Cluster {cluster_num}\")\n",
        "    plt.xlabel(\"Percentage of Time (%)\")\n",
        "    plt.ylabel(\"Load (Normalized)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to generate LDCs for all clusters without altering original code\n",
        "def generate_ldcs_for_clusters(year, best_kmeans, labels, school_names, cluster_data_dict):\n",
        "    \"\"\"\n",
        "    Generates Load Duration Curves (LDCs) for all clusters in a given year.\n",
        "\n",
        "    Parameters:\n",
        "    - year: int, the year of the data.\n",
        "    - best_kmeans: sklearn.cluster.KMeans object, the trained KMeans model.\n",
        "    - labels: array-like, cluster labels for each school.\n",
        "    - school_names: list of school names corresponding to the data points.\n",
        "    - cluster_data_dict: dict, pre-collected cluster data where the key is the cluster number,\n",
        "                         and the value is a pandas.Series or array of 'Normalized' load values.\n",
        "\n",
        "    Returns:\n",
        "    None: Displays the LDC plots for all clusters.\n",
        "    \"\"\"\n",
        "    print(f\"Generating LDCs for all clusters in year {year}...\")\n",
        "\n",
        "    for cluster in range(best_kmeans.n_clusters):\n",
        "        # Ensure the cluster data is available\n",
        "        if cluster in cluster_data_dict and not cluster_data_dict[cluster].empty:\n",
        "            generate_load_duration_curve(\n",
        "                cluster_data_dict[cluster],\n",
        "                title=f\"Cluster {cluster + 1} Load Duration Curve\",\n",
        "                year=year,\n",
        "                cluster_num=cluster + 1,\n",
        "            )\n",
        "        else:\n",
        "            print(f\"No data available for Cluster {cluster + 1} in year {year}.\")\n",
        "############################################ Load duration curves ##############################################\n",
        "\n",
        "\n",
        "############################################# Heat Map #########################################################\n",
        "def plot_heatmap(normalized_df, year, title=\"Heat Map - Load Profile\", cluster_num=None):\n",
        "    \"\"\"\n",
        "    Generates and plots a heat map (Day of the Year vs. Hour of the Day)\n",
        "    for normalized load values.\n",
        "\n",
        "    Parameters:\n",
        "    - normalized_df: pandas.DataFrame with columns ['Timestamp', 'Normalized'].\n",
        "    - year: int, the year of the data (for annotation).\n",
        "    - title: str, title for the heat map.\n",
        "    - cluster_num: int, optional cluster number for annotation.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    if normalized_df.empty:\n",
        "        print(\"Heatmap: The DataFrame is empty. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    # Validate required columns\n",
        "    if 'Timestamp' not in normalized_df.columns or 'Normalized' not in normalized_df.columns:\n",
        "        print(\"Heatmap: Missing 'Timestamp' or 'Normalized' column in DataFrame.\")\n",
        "        return\n",
        "\n",
        "    # Ensure 'Timestamp' is datetime\n",
        "    try:\n",
        "        normalized_df['Timestamp'] = pd.to_datetime(normalized_df['Timestamp'], errors='coerce')\n",
        "    except Exception as e:\n",
        "        print(f\"Heatmap: Error converting 'Timestamp' to datetime - {e}\")\n",
        "        return\n",
        "\n",
        "    # Drop invalid timestamps\n",
        "    normalized_df = normalized_df.dropna(subset=['Timestamp'])\n",
        "\n",
        "    if normalized_df.empty:\n",
        "        print(\"Heatmap: All timestamps are invalid after conversion.\")\n",
        "        return\n",
        "\n",
        "    # Extract day of the year and hour of the day\n",
        "    normalized_df['Hour'] = normalized_df['Timestamp'].dt.hour\n",
        "    normalized_df['DayOfYear'] = normalized_df['Timestamp'].dt.dayofyear\n",
        "\n",
        "    # Debugging: Check unique days and hours\n",
        "    print(f\"Heatmap: Data contains {normalized_df['DayOfYear'].nunique()} days and {normalized_df['Hour'].nunique()} hours.\")\n",
        "\n",
        "    # Create pivot table for heat map\n",
        "    heatmap_data = normalized_df.pivot_table(\n",
        "        index='DayOfYear',  # Rows: Day of the year\n",
        "        columns='Hour',     # Columns: Hour of the day\n",
        "        values='Normalized',  # Values: Normalized load\n",
        "        aggfunc='mean'       # Aggregation: Average\n",
        "    )\n",
        "\n",
        "    if heatmap_data.empty:\n",
        "        print(\"Heatmap: Pivot table is empty. Cannot generate heat map.\")\n",
        "        return\n",
        "\n",
        "    # Plot heat map\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(\n",
        "        heatmap_data,\n",
        "        cmap='coolwarm',\n",
        "        cbar_kws={'label': 'Normalized Load (W/m²)'},\n",
        "        linewidths=0.5\n",
        "    )\n",
        "    plt.title(f\"{title} - Year {year}{f' - Cluster {cluster_num}' if cluster_num else ''}\")\n",
        "    plt.xlabel(\"Hour of the Day\")\n",
        "    plt.ylabel(\"Day of the Year\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "############################################# Heat Map #########################################################\n",
        "\n",
        "# Filter data from the first Monday of January to the last Sunday of February\n",
        "def filter_january_february(df, year):\n",
        "    start_date = pd.to_datetime(f'{year}-01-01')\n",
        "    # Adjust start date to the first Monday of the year if necessary\n",
        "    if start_date.weekday() != 0:  # Not a Monday\n",
        "        start_date += pd.DateOffset(days=(7 - start_date.weekday()))\n",
        "    end_date = pd.to_datetime(f'{year}-02-28')\n",
        "    # Adjust end date to the last Sunday in February if necessary\n",
        "    if end_date.weekday() != 6:  # Not a Sunday\n",
        "        end_date += pd.DateOffset(days=(6 - end_date.weekday()))\n",
        "\n",
        "    return df[(df.iloc[:, 1] >= start_date) & (df.iloc[:, 1] <= end_date)]\n",
        "\n",
        "# Extract features based on energy consumption and mechanical equipment presence\n",
        "def extract_features(year):\n",
        "    features = []\n",
        "    school_names = []\n",
        "\n",
        "    for school_name in floorAreas.keys():\n",
        "        floor_area = floorAreas[school_name]\n",
        "        schoolPath = f\"/content/drive/MyDrive/🍁 /UdS/Thesis/Projects/{school_name}\"\n",
        "\n",
        "        # Ensure directory exists\n",
        "        if not os.path.isdir(schoolPath):\n",
        "            print(f\"Warning: Directory {schoolPath} does not exist.\")\n",
        "            continue\n",
        "\n",
        "        csv_files = [f for f in os.listdir(schoolPath) if f.endswith('.csv')]\n",
        "        energy_data = []\n",
        "        for file in csv_files:\n",
        "            full_path = os.path.join(schoolPath, file)\n",
        "            try:\n",
        "                df = pd.read_csv(full_path, delimiter=';', decimal=',')\n",
        "                df.iloc[:, 1] = pd.to_datetime(df.iloc[:, 1], errors='coerce')\n",
        "                df = df.dropna(subset=[df.columns[1]])\n",
        "\n",
        "                if df.shape[1] >= 3:\n",
        "                    df.iloc[:, 2] = pd.to_numeric(df.iloc[:, 2], errors='coerce')\n",
        "                    df = df.dropna(subset=[df.columns[2]])\n",
        "\n",
        "                    df_filtered = filter_january_february(df, year)\n",
        "                    if not df_filtered.empty:\n",
        "                        normalized_values = df_filtered.iloc[:, 2] / floor_area\n",
        "                        avg_consumption = normalized_values.mean()\n",
        "                        peak_consumption = normalized_values.max()\n",
        "                        energy_data.append([avg_consumption, peak_consumption])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file} in {schoolPath}: {e}\")\n",
        "\n",
        "        if energy_data:\n",
        "            avg_energy = np.mean([d[0] for d in energy_data])\n",
        "            peak_energy = np.mean([d[1] for d in energy_data])\n",
        "\n",
        "            equip_info = equipment.get(school_name, {})\n",
        "            equip_features = [1 if equip_info.get(key, 0) > 0 else 0 for key in [\"GB\", \"EB\", \"TES\", \"HP\", \"GT\"]]\n",
        "\n",
        "            features.append([avg_energy, peak_energy] + equip_features)\n",
        "            school_names.append(school_name)\n",
        "\n",
        "    print(f\"Extracted features for year {year}: {features}\")  # Debug line to verify feature extraction\n",
        "    return np.array(features), school_names\n",
        "\n",
        "# Perform K-Means clustering with silhouette score optimization\n",
        "def optimized_kmeans(features, n_clusters_range=(2, 10)):\n",
        "    best_score = -1\n",
        "    best_kmeans = None\n",
        "    best_labels = None\n",
        "    silhouette_scores = []  # Initialize a list to store silhouette scores\n",
        "\n",
        "    for n_clusters in range(n_clusters_range[0], min(n_clusters_range[1], len(features))):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        labels = kmeans.fit_predict(features)\n",
        "        score = silhouette_score(features, labels)\n",
        "        silhouette_scores.append((n_clusters, score))  # Append score to the list\n",
        "        if score > best_score:\n",
        "            best_score, best_kmeans, best_labels = score, kmeans, labels\n",
        "    return best_kmeans, best_labels, silhouette_scores  # Return silhouette scores\n",
        "\n",
        "# Interpret the cluster's mechanical equipment based on centroid values\n",
        "def interpret_cluster_equipment(centroid):\n",
        "    equipment_types = [\"GB\", \"EB\", \"TES\", \"HP\", \"GT\"]\n",
        "    interpretation = ', '.join([equipment_types[i] for i, val in enumerate(centroid[2:]) if val >= 0.5])\n",
        "    print(f\"Cluster equipment interpretation: {interpretation}\")  # Debug line\n",
        "    return interpretation\n",
        "\n",
        "#############################################################################################################\n",
        "\n",
        "# Function to generate all weekday and weekend timestamps at 8 AM in January and February\n",
        "def generate_jan_feb_timestamps(year):\n",
        "    jan_feb_weekdays = []\n",
        "    jan_feb_weekends = []\n",
        "    start_date = datetime(year, 1, 1)\n",
        "    end_date = datetime(year, 2, 28)\n",
        "\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        time_at_8am = current_date.replace(hour=8, minute=0, second=0)\n",
        "        if current_date.weekday() < 5:  # Monday=0, Friday=4 (weekdays)\n",
        "            jan_feb_weekdays.append(time_at_8am)\n",
        "        else:  # Saturday=5, Sunday=6 (weekends)\n",
        "            jan_feb_weekends.append(time_at_8am)\n",
        "        current_date += timedelta(days=1)\n",
        "    return jan_feb_weekdays, jan_feb_weekends\n",
        "\n",
        "# Complete cluster_and_plot function with peak load detection at 8 AM for weekdays and weekends in January and February\n",
        "def cluster_and_plot(year):\n",
        "    features, school_names = extract_features(year)\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    best_kmeans, labels, silhouette_scores = optimized_kmeans(features_scaled)\n",
        "\n",
        "    # Plot Silhouette scores\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    n_clusters, scores = zip(*silhouette_scores)\n",
        "    plt.plot(n_clusters, scores, marker='o', linestyle='-', color='b')\n",
        "    plt.title(f'Silhouette Scores for Different Numbers of Clusters - Year {year}')\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    centroids = scaler.inverse_transform(best_kmeans.cluster_centers_)\n",
        "\n",
        "\n",
        "    # Generate timestamps for all weekdays and weekends at 8 AM in January and February\n",
        "    specific_weekday_times, specific_weekend_times = generate_jan_feb_timestamps(year)\n",
        "\n",
        "    print(f\"Total clusters for {year}: {best_kmeans.n_clusters}\")  # Debug line\n",
        "\n",
        "    # Dictionaries to store peak loads for each cluster for combined plotting\n",
        "    cluster_weekday_peak_loads = {}\n",
        "    cluster_weekend_peak_loads = {}\n",
        "\n",
        "    for cluster in range(best_kmeans.n_clusters):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        cluster_schools = [school_names[i] for i, label in enumerate(labels) if label == cluster]\n",
        "\n",
        "        print(f\"Cluster {cluster + 1} for {year} contains schools: {cluster_schools}\")  # Debug line\n",
        "\n",
        "        legend_added = set()\n",
        "        all_schools_data = []  # Collect data to calculate the average for the cluster\n",
        "        cluster_data = pd.DataFrame()  # For typical week calculation\n",
        "\n",
        "        # Lists to collect peak loads at 8 AM on specified weekdays and weekends in Jan-Feb for this cluster\n",
        "        weekday_peak_loads_for_distribution = []\n",
        "        weekend_peak_loads_for_distribution = []\n",
        "\n",
        "        for school_name in cluster_schools:\n",
        "            schoolPath = f\"/content/drive/MyDrive/🍁 /UdS/Thesis/Projects/{school_name}\"\n",
        "            floor_area = floorAreas[school_name]\n",
        "            color = school_colors.get(school_name, 'black')\n",
        "            csv_files = [f for f in os.listdir(schoolPath) if f.endswith('.csv')]\n",
        "\n",
        "            # Get equipment types for the school\n",
        "            equipment_types = equipment.get(school_name, {})\n",
        "            equipment_str = \", \".join([key for key, count in equipment_types.items() if count > 0])\n",
        "            legend_label = f\"{school_name}: {equipment_str}\"\n",
        "\n",
        "            school_data = pd.DataFrame()\n",
        "            for file in csv_files:\n",
        "                full_path = os.path.join(schoolPath, file)\n",
        "                try:\n",
        "                    df = pd.read_csv(full_path, delimiter=';', decimal=',')\n",
        "                    df.iloc[:, 1] = pd.to_datetime(df.iloc[:, 1], errors='coerce')\n",
        "                    df = df.dropna(subset=[df.columns[1]])\n",
        "\n",
        "                    if df.shape[1] >= 3:\n",
        "                        df.iloc[:, 2] = pd.to_numeric(df.iloc[:, 2], errors='coerce')\n",
        "                        df = df.dropna(subset=[df.columns[2]])\n",
        "\n",
        "                        df_filtered = filter_january_february(df, year)\n",
        "                        if not df_filtered.empty:\n",
        "                            # Normalize data and add to school data\n",
        "                            normalized_values = (df_filtered.iloc[:, 2] / floor_area) * 1000\n",
        "                            df_filtered['Normalized'] = normalized_values\n",
        "                            school_data = pd.concat([school_data, df_filtered[['Normalized', df.columns[1]]]])\n",
        "                            label = legend_label if school_name not in legend_added else None\n",
        "                            plt.plot(df_filtered.iloc[:, 1], normalized_values, label=label, color=color)\n",
        "                            legend_added.add(school_name)\n",
        "\n",
        "                            # Collect peak loads at 8 AM on specified weekdays and weekends in January-February\n",
        "                            for peak_time in specific_weekday_times:\n",
        "                                peak_value = df_filtered[df_filtered[df.columns[1]] == peak_time]\n",
        "                                if not peak_value.empty:\n",
        "                                    weekday_peak_loads_for_distribution.append(peak_value['Normalized'].values[0])\n",
        "                            for peak_time in specific_weekend_times:\n",
        "                                peak_value = df_filtered[df_filtered[df.columns[1]] == peak_time]\n",
        "                                if not peak_value.empty:\n",
        "                                    weekend_peak_loads_for_distribution.append(peak_value['Normalized'].values[0])\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {file} in {schoolPath}: {e}\")\n",
        "\n",
        "            # Collect data for cluster average and typical week\n",
        "            if not school_data.empty:\n",
        "                school_data.set_index(df.columns[1], inplace=True)\n",
        "                school_data = school_data.resample('15T').mean()  # Resample to 15-minute intervals\n",
        "                all_schools_data.append(school_data['Normalized'])\n",
        "                cluster_data = pd.concat([cluster_data, school_data['Normalized']])\n",
        "\n",
        "        # Plot average load profile for the cluster\n",
        "        if all_schools_data:\n",
        "            cluster_avg = pd.concat(all_schools_data, axis=1).mean(axis=1)\n",
        "            plt.plot(cluster_avg.index, cluster_avg, label=\"Cluster Average\", color=\"black\", linestyle=\"--\", linewidth=2)\n",
        "\n",
        "        plt.title(f'Cluster {cluster + 1} - {year}')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('W/m²')\n",
        "        plt.legend(title=\"Schools\")\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Store peak loads for combined plots\n",
        "        if weekday_peak_loads_for_distribution:\n",
        "            cluster_weekday_peak_loads[cluster + 1] = weekday_peak_loads_for_distribution\n",
        "        if weekend_peak_loads_for_distribution:\n",
        "            cluster_weekend_peak_loads[cluster + 1] = weekend_peak_loads_for_distribution\n",
        "\n",
        "        # Individual \"statistical distribution of peak load\" for each cluster on weekdays\n",
        "        if weekday_peak_loads_for_distribution:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            counts, bins, _ = plt.hist(weekday_peak_loads_for_distribution, bins=15, color=\"skyblue\", edgecolor=\"black\", alpha=0.7, density=True)\n",
        "\n",
        "            # Gaussian Fit\n",
        "            mu, sigma = np.mean(weekday_peak_loads_for_distribution), np.std(weekday_peak_loads_for_distribution)\n",
        "            x = np.linspace(min(weekday_peak_loads_for_distribution), max(weekday_peak_loads_for_distribution), 100)\n",
        "            plt.plot(x, norm.pdf(x, mu, sigma), color=\"red\", linestyle=\"--\", linewidth=2, label=f'Gaussian Fit\\nμ={mu:.2f}, σ={sigma:.2f}')\n",
        "\n",
        "            plt.title(f'Statistical Distribution of Peak Load at 8 AM on Jan-Feb Weekdays - Cluster {cluster + 1} - {year}')\n",
        "            plt.xlabel(\"Normalized Peak Load (W/m²)\")\n",
        "            plt.ylabel(\"Density\")\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        # Individual \"statistical distribution of peak load\" for each cluster on weekends\n",
        "        if weekend_peak_loads_for_distribution:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            counts, bins, _ = plt.hist(weekend_peak_loads_for_distribution, bins=15, color=\"lightcoral\", edgecolor=\"black\", alpha=0.7, density=True)\n",
        "\n",
        "            # Gaussian Fit\n",
        "            mu, sigma = np.mean(weekend_peak_loads_for_distribution), np.std(weekend_peak_loads_for_distribution)\n",
        "            x = np.linspace(min(weekend_peak_loads_for_distribution), max(weekend_peak_loads_for_distribution), 100)\n",
        "            plt.plot(x, norm.pdf(x, mu, sigma), color=\"blue\", linestyle=\"--\", linewidth=2, label=f'Gaussian Fit\\nμ={mu:.2f}, σ={sigma:.2f}')\n",
        "\n",
        "            plt.title(f'Statistical Distribution of Peak Load at 8 AM on Jan-Feb Weekends - Cluster {cluster + 1} - {year}')\n",
        "            plt.xlabel(\"Normalized Peak Load (W/m²)\")\n",
        "            plt.ylabel(\"Density\")\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        # Typical week and day plots remain unchanged as in your original requirements...\n",
        "\n",
        "    # Combined \"statistical distribution of peak load\" for all clusters on weekdays\n",
        "    if cluster_weekday_peak_loads:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        colors = plt.cm.tab10.colors\n",
        "        for i, (cluster_num, peak_loads) in enumerate(cluster_weekday_peak_loads.items()):\n",
        "            counts, bins, _ = plt.hist(peak_loads, bins=15, color=colors[i % len(colors)], edgecolor=\"black\", alpha=0.5, density=True, label=f'Cluster {cluster_num}')\n",
        "\n",
        "            # Gaussian Fit for each cluster\n",
        "            mu, sigma = np.mean(peak_loads), np.std(peak_loads)\n",
        "            x = np.linspace(min(peak_loads), max(peak_loads), 100)\n",
        "            plt.plot(x, norm.pdf(x, mu, sigma), color=colors[i % len(colors)], linestyle=\"--\", linewidth=2, label=f'Cluster {cluster_num} Fit\\nμ={mu:.2f}, σ={sigma:.2f}')\n",
        "\n",
        "        plt.title(f'Combined Statistical Distribution of Peak Load at 8 AM on Jan-Feb Weekdays - Year {year}')\n",
        "        plt.xlabel(\"Normalized Peak Load (W/m²)\")\n",
        "        plt.ylabel(\"Density\")\n",
        "        plt.legend(title=\"Clusters\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Combined \"statistical distribution of peak load\" for all clusters on weekends\n",
        "    if cluster_weekend_peak_loads:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        for i, (cluster_num, peak_loads) in enumerate(cluster_weekend_peak_loads.items()):\n",
        "            counts, bins, _ = plt.hist(peak_loads, bins=15, color=colors[i % len(colors)], edgecolor=\"black\", alpha=0.5, density=True, label=f'Cluster {cluster_num}')\n",
        "\n",
        "            # Gaussian Fit for each cluster\n",
        "            mu, sigma = np.mean(peak_loads), np.std(peak_loads)\n",
        "            x = np.linspace(min(peak_loads), max(peak_loads), 100)\n",
        "            plt.plot(x, norm.pdf(x, mu, sigma), color=colors[i % len(colors)], linestyle=\"--\", linewidth=2, label=f'Cluster {cluster_num} Fit\\nμ={mu:.2f}, σ={sigma:.2f}')\n",
        "\n",
        "        plt.title(f'Combined Statistical Distribution of Peak Load at 8 AM on Jan-Feb Weekends - Year {year}')\n",
        "        plt.xlabel(\"Normalized Peak Load (W/m²)\")\n",
        "        plt.ylabel(\"Density\")\n",
        "        plt.legend(title=\"Clusters\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "############################################ Load duration curves ##############################################\n",
        "# Modified cluster_and_plot to Collect Cluster Data\n",
        "def cluster_and_plot(year):\n",
        "    features, school_names = extract_features(year)\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    best_kmeans, labels, silhouette_scores = optimized_kmeans(features_scaled)\n",
        "\n",
        "    cluster_data_dict = {}  # Collect cluster data for LDCs\n",
        "\n",
        "    for cluster in range(best_kmeans.n_clusters):\n",
        "        cluster_data = pd.DataFrame()\n",
        "\n",
        "        for school_name in [school_names[i] for i, label in enumerate(labels) if label == cluster]:\n",
        "            schoolPath = f\"/content/drive/MyDrive/🍁 /UdS/Thesis/Projects/{school_name}\"\n",
        "            floor_area = floorAreas[school_name]\n",
        "            csv_files = [f for f in os.listdir(schoolPath) if f.endswith('.csv')]\n",
        "\n",
        "            for file in csv_files:\n",
        "                full_path = os.path.join(schoolPath, file)\n",
        "                try:\n",
        "                    df = pd.read_csv(full_path, delimiter=';', decimal=',')\n",
        "                    df.iloc[:, 1] = pd.to_datetime(df.iloc[:, 1], errors='coerce')\n",
        "                    df = df.dropna(subset=[df.columns[1]])\n",
        "\n",
        "                    if df.shape[1] >= 3:\n",
        "                        df.iloc[:, 2] = pd.to_numeric(df.iloc[:, 2], errors='coerce')\n",
        "                        df = df.dropna(subset=[df.columns[2]])\n",
        "\n",
        "                        df_filtered = filter_january_february(df, year)\n",
        "                        if not df_filtered.empty:\n",
        "                            normalized_values = (df_filtered.iloc[:, 2] / floor_area) * 1000\n",
        "                            df_filtered['Normalized'] = normalized_values\n",
        "                            cluster_data = pd.concat([cluster_data, df_filtered[['Normalized', df.columns[1]]]])\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {file} in {schoolPath}: {e}\")\n",
        "\n",
        "        if not cluster_data.empty:\n",
        "            cluster_data_dict[cluster] = cluster_data['Normalized']\n",
        "\n",
        "    # Generate LDCs for clusters\n",
        "    generate_ldcs_for_clusters(year, best_kmeans, labels, school_names, cluster_data_dict)\n",
        "############################################ Load duration curves ##############################################\n",
        "\n",
        "\n",
        "############################################# Heat Map #########################################################\n",
        "############################################# Heat Map #########################################################\n",
        "\n",
        "\n",
        "# Run for both years\n",
        "cluster_and_plot(2023)\n",
        "cluster_and_plot(2024)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhiF9yvyvdJd"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6yCiBnWq61NSfD2KammCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}